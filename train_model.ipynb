{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalaciones\n",
    "\n",
    "%pip install torch\n",
    "%pip install open3d\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Using cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/modelNet10classifier/model.py:309: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "## Dependencias\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "import os\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import PointNetClassifier, PointNetLoss, PointNetKAN, TTAClassifier\n",
    "from modelnet10 import ModelNetClass, ModelNet, DatasetType\n",
    "from utils.csv import save_loss_dict\n",
    "from utils.transformation import (Normalization,\n",
    "                                  Rotation, Translation, Reflection, Scale,\n",
    "                                  DropRandom, DropSphere, Jittering, Noise)\n",
    "from trainer import PointNetTrainer\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros globales\n",
    "checkpoint_freq = 25\n",
    "\n",
    "# parámetros del dataset\n",
    "classes = [label for label in ModelNetClass]\n",
    "batch_size = 32\n",
    "dim = 3\n",
    "num_points = 1024\n",
    "num_classes = len(classes)\n",
    "\n",
    "# hiperparámetros\n",
    "num_global_feats = 1024     # número de features globales calculadas\n",
    "learning_rate = 0.001\n",
    "reg_weight = 0.001\n",
    "gamma = 2                   # Recomendado por el paper de focal loss\n",
    "\n",
    "# TODO: Más adelante usar alpha para clases imbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset de entrenamiento\n",
    "t = [Rotation(), Reflection(), Scale(max_ratio=2.5),\n",
    "    Jittering(max_units=0.005), DropRandom(loss_ratio=0.4), Noise()]\n",
    "\n",
    "train_data = ModelNet(classes, DatasetType.TRAIN, repetitions=3, transformations=t, preserve_original=False)\n",
    "validation_data = ModelNet(classes, DatasetType.VALIDATION, repetitions=3, transformations=t, preserve_original=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "def train(\n",
    "        epochs: int,\n",
    "        name: str,\n",
    "        num_global_feats: int,\n",
    "        learning_rate: int,\n",
    "        use_scheduler: bool,\n",
    "        alpha: List[int],\n",
    "        gamma: int,\n",
    "        reg_weight: int,\n",
    "        use_kan: bool,\n",
    "        ignore_Tnet: bool,\n",
    "        norm_type: str,\n",
    "        dropout: float\n",
    "):\n",
    "    if not use_kan:\n",
    "        classifier = PointNetClassifier(dim, num_points, num_global_feats, num_classes, ignore_Tnet=ignore_Tnet,\n",
    "                                        norm_type=norm_type, dropout=dropout).to(DEVICE)\n",
    "    else:\n",
    "        classifier = PointNetKAN(dim, num_points, num_classes, scaling = 2.0).to(DEVICE)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    if DEVICE == \"cuda\" and use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=2000, cycle_momentum=False)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    trainer = PointNetTrainer(\n",
    "        name=name,\n",
    "        model=classifier,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=reg_weight, size_average=True).to(DEVICE),\n",
    "        device=DEVICE,\n",
    "        train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True),\n",
    "        val_loader=DataLoader(validation_data, batch_size=batch_size, shuffle=False),\n",
    "        checkpoint_dir=os.path.join(os.getcwd(), \"checkpoint\"),\n",
    "        checkpoint_freq=checkpoint_freq\n",
    "    )\n",
    "\n",
    "    loss_dict, best_epoch, best_loss, best_acc = trainer.fit(epochs=epochs)\n",
    "    #save_loss_dict(loss_dict, os.path.join(os.getcwd(), \"csv\", f\"{name}_loss_dict.csv\"))\n",
    "    print(f\"{name} | Best model @ epoch {best_epoch}: loss = {best_loss:.4f}, acc = {best_acc:.4f}\")\n",
    "\n",
    "# Instancias de entrenamiento\n",
    "EPOCHS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupnorm with hard regularization: reg = 0.005 dp = 0.5\n",
    "train(epochs=EPOCHS, name=\"gnregreg\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=False, alpha=None, gamma=2, reg_weight=0.005, use_kan=False, ignore_Tnet=False,\n",
    "      norm_type=\"groupnorm\", dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupnorm with hard regularization and alpha inv freq\n",
    "alpha = [3991/106, 3991/515, 3991/889, 3991/200, 3991/200, 3991/465, 3991/200, 3991/680, 3991/392, 3991/344]\n",
    "train(epochs=EPOCHS, name=\"gnregregalpha\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=False, alpha=alpha, gamma=2, reg_weight=0.005, use_kan=False, ignore_Tnet=False,\n",
    "      norm_type=\"groupnorm\", dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layernorm with hard regularization: reg = 0.005 dp = 0.5\n",
    "train(epochs=EPOCHS, name=\"lnregreg\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=False, alpha=None, gamma=2, reg_weight=0.005, use_kan=False, ignore_Tnet=False,\n",
    "      norm_type=\"layernorm\", dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset de prueba\n",
    "base_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[])\n",
    "affine_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[Rotation(), Reflection(), Scale(max_ratio=2.5)])\n",
    "complex_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[Rotation(), Reflection(), Scale(max_ratio=2.5),\n",
    "                                          Jittering(max_units=0.005), DropRandom(loss_ratio=0.4), Noise()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_it(classifier_path: str, num_global_feats=num_global_feats, use_kan=False,\n",
    "            ignore_Tnet=False, use_TTA=False, merge_mode=None, norm_type=\"batchnorm\"):\n",
    "\n",
    "    for data_name, data in [[\"base\", base_test_data], [\"affine\", affine_test_data], [\"complex\", complex_test_data]]:\n",
    "        data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        if not use_kan:\n",
    "            classifier = PointNetClassifier(dim, num_points, num_global_feats, num_classes,\n",
    "                                            ignore_Tnet=ignore_Tnet, norm_type=norm_type).to(DEVICE)\n",
    "        else:\n",
    "            classifier = PointNetKAN(dim, num_points, num_classes, scaling = 1.0, ignore_Tnet=ignore_Tnet).to(DEVICE)\n",
    "        \n",
    "        \n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=torch.device(DEVICE)))\n",
    "\n",
    "        if use_TTA:\n",
    "            classifier = TTAClassifier(classifier=classifier, transformations=[Rotation(), Reflection()], merge_mode=merge_mode)\n",
    "\n",
    "        \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            classifier = classifier.eval()\n",
    "            correct = 0\n",
    "            \n",
    "            for pcds, labels in data_loader:\n",
    "                pcds = pcds.to(DEVICE)\n",
    "                labels = labels.squeeze().to(DEVICE)\n",
    "\n",
    "                # Hacer predicciones\n",
    "                out, _, _ = classifier(pcds)\n",
    "\n",
    "                if not use_TTA:\n",
    "                    # Calculamos las elecciones, TTA ya hace softmax.\n",
    "                    out = torch.softmax(out, dim=1)\n",
    "                    \n",
    "                pred_choice = out.argmax(dim=1)\n",
    "                \n",
    "                # Elecciones correctas, acumuladas\n",
    "                correct += pred_choice.eq(labels.data).cpu().sum().item()\n",
    "\n",
    "            test_acc = correct / float(len(data))\n",
    "            print(f\"\\tAccuracy on {data_name} dataset:\\t\", test_acc)\n",
    "\n",
    "# Tests\n",
    "_dir = os.path.join(os.getcwd(), \"checkpoint\", \"best_model\")\n",
    "print(\"Groupnorm classifier (best epoch):\")\n",
    "test_it(os.path.join(_dir, \"gn_best_model.pth\"), norm_type=\"groupnorm\")\n",
    "print(\"Less dropout classifier (best epoch):\")\n",
    "test_it(os.path.join(_dir, \"lessdp_best_model.pth\"))\n",
    "print(\"More dropout classifier (best epoch):\")\n",
    "test_it(os.path.join(_dir, \"moredp_best_model.pth\"))\n",
    "print(\"Groupnorm with regularization classifier (best epoch):\")\n",
    "test_it(os.path.join(_dir, \"gnreg_best_model.pth\"), norm_type=\"groupnorm\")\n",
    "\n",
    "print(\"Groupnorm classifier (last epoch):\")\n",
    "test_it(os.path.join(os.getcwd(), \"checkpoint\", \"by_epoch\", \"gn_epoch_0200.pth\"), norm_type=\"groupnorm\")\n",
    "print(\"Less dropout classifier (last epoch):\")\n",
    "test_it(os.path.join(os.getcwd(), \"checkpoint\", \"by_epoch\", \"lessdp_epoch_0200.pth\"))\n",
    "print(\"More dropout classifier (last epoch):\")\n",
    "test_it(os.path.join(os.getcwd(), \"checkpoint\", \"by_epoch\", \"moredp_epoch_0200.pth\"))\n",
    "print(\"Groupnorm with regularization classifier (last epoch):\")\n",
    "test_it(os.path.join(os.getcwd(), \"checkpoint\", \"by_epoch\", \"gnreg_epoch_0200.pth\"), norm_type=\"groupnorm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
