{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instalaciones\n",
    "\n",
    "%pip install torch\n",
    "%pip install open3d\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencias\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "import os\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import PointNetClassifier, PointNetLoss, PointNetKAN\n",
    "from modelnet10 import ModelNetClass, ModelNet, DatasetType\n",
    "from utils.csv import save_loss_dict\n",
    "from utils.transformation import (Normalization,\n",
    "                                  Rotation, Translation, Reflection, Scale,\n",
    "                                  DropRandom, DropSphere, Jittering, Noise)\n",
    "from trainer import PointNetTrainer\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros globales\n",
    "checkpoint_freq = 25\n",
    "\n",
    "# parámetros del dataset\n",
    "classes = [label for label in ModelNetClass]\n",
    "batch_size = 32\n",
    "dim = 3\n",
    "num_points = 1024\n",
    "num_classes = len(classes)\n",
    "\n",
    "# hiperparámetros\n",
    "num_global_feats = 1024     # número de features globales calculadas\n",
    "learning_rate = 0.001\n",
    "reg_weight = 0.001\n",
    "gamma = 2                   # Recomendado por el paper de focal loss\n",
    "\n",
    "# dataset de entrenamiento\n",
    "t = [Rotation(), Reflection(), Scale(max_ratio=2.5),\n",
    "    Jittering(max_units=0.005), DropRandom(loss_ratio=0.4), Noise()]\n",
    "\n",
    "train_data = ModelNet(classes, DatasetType.TRAIN, repetitions=3, transformations=t, preserve_original=False)\n",
    "validation_data = ModelNet(classes, DatasetType.VALIDATION, repetitions=3, transformations=t, preserve_original=False)\n",
    "    \n",
    "# TODO: Más adelante usar alpha para clases imbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "def train(\n",
    "        epochs: int,\n",
    "        name: str,\n",
    "        num_global_feats: int,\n",
    "        learning_rate: int,\n",
    "        use_scheduler: bool,\n",
    "        alpha: List[int],\n",
    "        gamma: int,\n",
    "        reg_weight: int,\n",
    "        use_kan: bool,\n",
    "        ignore_Tnet: bool,\n",
    "):\n",
    "    if not use_kan:\n",
    "        classifier = PointNetClassifier(dim, num_points, num_global_feats, num_classes, ignore_Tnet=ignore_Tnet).to(DEVICE)\n",
    "    else:\n",
    "        classifier = PointNetKAN(dim, num_points, num_classes, scaling = 2.0).to(DEVICE)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    if DEVICE == \"cuda\" and use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=2000, cycle_momentum=False)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    trainer = PointNetTrainer(\n",
    "        name=name,\n",
    "        model=classifier,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=reg_weight, size_average=True).to(DEVICE),\n",
    "        device=DEVICE,\n",
    "        train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True),\n",
    "        val_loader=DataLoader(validation_data, batch_size=batch_size, shuffle=False),\n",
    "        checkpoint_dir=os.path.join(os.getcwd(), \"checkpoint\"),\n",
    "        checkpoint_freq=checkpoint_freq\n",
    "    )\n",
    "\n",
    "    loss_dict, best_epoch, best_loss, best_acc = trainer.fit(epochs=epochs)\n",
    "    #save_loss_dict(loss_dict, os.path.join(os.getcwd(), \"csv\", f\"{name}_loss_dict.csv\"))\n",
    "    print(f\"{name} | Best model @ epoch {best_epoch}: loss = {best_loss:.4f}, acc = {best_acc:.4f}\")\n",
    "\n",
    "# Instancias de entrenamiento\n",
    "EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=EPOCHS, name=\"base\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=False, alpha=None, gamma=0, reg_weight=reg_weight, use_kan=False, ignore_Tnet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [3991/106, 3991/515, 3991/889, 3991/200, 3991/200, 3991/465, 3991/200, 3991/680, 3991/392, 3991/344]\n",
    "train(epochs=EPOCHS, name=\"mod\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=True, alpha=alpha, gamma=gamma, reg_weight=reg_weight, use_kan=False, ignore_Tnet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=EPOCHS, name=\"no_tnet\", num_global_feats=num_global_feats, learning_rate=learning_rate,\n",
    "      use_scheduler=False, alpha=None, gamma=0, reg_weight=0, use_kan=False, ignore_Tnet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset de prueba\n",
    "base_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[])\n",
    "affine_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[Rotation(), Reflection(), Scale(max_ratio=2.5)])\n",
    "complex_test_data = ModelNet(classes, DatasetType.TEST, repetitions=1, preserve_original=False,\n",
    "                          transformations=[Rotation(), Reflection(), Scale(max_ratio=2.5),\n",
    "                                          Jittering(max_units=0.005), DropRandom(loss_ratio=0.4), Noise()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_it(classifier_path: str, num_global_feats=num_global_feats, use_kan=False, ignore_Tnet=False):\n",
    "\n",
    "    for data_name, data in [[\"base\", base_test_data], [\"affine\", affine_test_data], [\"complex\", complex_test_data]]:\n",
    "        data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        if not use_kan:\n",
    "            classifier = PointNetClassifier(dim, num_points, num_global_feats, num_classes, ignore_Tnet=ignore_Tnet).to(DEVICE)\n",
    "        else:\n",
    "            classifier = PointNetKAN(dim, num_points, num_classes, scaling = 2.0).to(DEVICE)\n",
    "        classifier.load_state_dict(torch.load(classifier_path))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            classifier = classifier.eval()\n",
    "            correct = 0\n",
    "            \n",
    "            for pcds, labels in data_loader:\n",
    "                pcds = pcds.to(DEVICE)\n",
    "                labels = labels.squeeze().to(DEVICE)\n",
    "                \n",
    "                # Hacer predicciones\n",
    "                out, _, _ = classifier(pcds)\n",
    "            \n",
    "                # Calculamos las elecciones\n",
    "                pred_choice = torch.softmax(out, dim=1).argmax(dim=1)\n",
    "                \n",
    "                # Elecciones correctas, acumuladas\n",
    "                correct += pred_choice.eq(labels.data).cpu().sum().item()\n",
    "\n",
    "            test_acc = correct / float(len(data))\n",
    "            print(f\"\\tAccuracy on {data_name} dataset:\\t\", test_acc)\n",
    "\n",
    "# Tests\n",
    "_dir = os.path.join(os.getcwd(), \"checkpoint\", \"best_model\")\n",
    "print(\"Base classifier:\")\n",
    "test_it(os.path.join(_dir, \"base_best_model.pth\"))\n",
    "print(\"Modified classifier with KAN, alpha, gamma, scheduler:\")\n",
    "test_it(os.path.join(_dir, \"mod_best_model.pth\"), use_kan=True)\n",
    "print(\"Base classifier without Tnet:\")\n",
    "test_it(os.path.join(_dir, \"no_tnet_best_model.pth\"), ignore_Tnet=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
